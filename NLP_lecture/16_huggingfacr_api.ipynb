{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 허깅 페이스 pipeline \n",
    "\n",
    "[추가 내용은 내일할게요~]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.34.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers \n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "감성 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998449087142944}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis')\n",
    "classifier(\"I love nlp!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개체명 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\ProgramData\\Anaconda3\\envs\\llm_env\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline('ner', grouped_entities = True)\n",
    "rs = pipe(\"Dokdo is a unique territory of Korea.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'LOC',\n",
       "  'score': 0.9633191,\n",
       "  'word': 'Dokdo',\n",
       "  'start': 0,\n",
       "  'end': 5},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9983676,\n",
       "  'word': 'Korea',\n",
       "  'start': 31,\n",
       "  'end': 36}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'MISC',\n",
       "  'score': 0.53468937,\n",
       "  'word': 'Do',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.8058966,\n",
       "  'word': '##kdo Korea',\n",
       "  'start': 2,\n",
       "  'end': 11},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99899143,\n",
       "  'word': 'Korea',\n",
       "  'start': 37,\n",
       "  'end': 42}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline('ner', grouped_entities = True)\n",
    "rs = pipe(\"Dokdo Korea is a unique territory of Korea.\")\n",
    "rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fill mask\n",
    "\n",
    "주어진 텍스트의 빈칸을 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6790189146995544,\n",
       "  'token': 812,\n",
       "  'token_str': ' capital',\n",
       "  'sequence': 'Paris is the capital of France.'},\n",
       " {'score': 0.05178016796708107,\n",
       "  'token': 32357,\n",
       "  'token_str': ' birthplace',\n",
       "  'sequence': 'Paris is the birthplace of France.'},\n",
       " {'score': 0.03825264424085617,\n",
       "  'token': 1144,\n",
       "  'token_str': ' heart',\n",
       "  'sequence': 'Paris is the heart of France.'},\n",
       " {'score': 0.02434876561164856,\n",
       "  'token': 29778,\n",
       "  'token_str': ' envy',\n",
       "  'sequence': 'Paris is the envy of France.'},\n",
       " {'score': 0.02285119704902172,\n",
       "  'token': 1867,\n",
       "  'token_str': ' Capital',\n",
       "  'sequence': 'Paris is the Capital of France.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"fill-mask\")\n",
    "classifier(\"Paris is the <mask> of France.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "질의 응답 \n",
    "\n",
    "* 주어진 지문(context)의 정보를 활용하여 질문에 대한 답을 하는 태스크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "question_answerer = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.7139148116111755, 'start': 31, 'end': 37, 'answer': 'cj.ctd'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer(\n",
    "    question = \"Where do I work?\",\n",
    "    context = \"My name is suyun and I work at cj.ctd in seoul\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "result = pipeline(\"question-answering\", model=\"klue/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0002701370685826987,\n",
       " 'start': 59,\n",
       " 'end': 72,\n",
       " 'answer': '된 이후 거의 오스트리아'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result(\n",
    "    question = \"베토벤이 태어난 곳은 어디인가요?\",\n",
    "    context = \"루트비히 판 베토벤은 독일의 서양 고전 음악 작곡가이자 피아니스트이다. 독일의 본에서 태어났으며, 성인이 된 이후 거의 오스트리아 빈에서 살았다. 감기와 폐렴의 합병증으로 투병하다가 57세로 세상을 떠난 그는 고전주의와 낭만주의의 전환기에 활동한 주요 음악가이며, 종종 영웅적인 인물로도 묘사된다. 음악의 성인 즉 악성이라는 별칭으로도 불린다..\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 생성\n",
    "\n",
    "* 프롬프트를 모델에 제공하면 모델이 나머지 텍스트를 생성하여 문장을 완성함\n",
    "* 텍스트 자동 완성 기능 유사\n",
    "* 텍스트 생성에는 랜덤하게 결과를 생성하는 과정이 포함되어 매번 다른 결과 도출에 주의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 41.4kB/s]\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to design a simple \"do we?\" design for your business. When you are ready to dive into the world of UX design, we can help you discover how you can build smart and powerful software that makes'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to create smart, fun (and scary, silly) jokes.\\n\\nHow to Write Fun and Funny'},\n",
       " {'generated_text': 'In this course, we will teach you how to do a Google Hangout with a friend, and you will learn to use these tools in conjunction with'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\n",
    "        \"In this course, we will teach you how to\",\n",
    "        max_length = 30 ,\n",
    "        num_return_sequences = 2\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한글 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 513M/513M [00:35<00:00, 14.3MB/s] \n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.83M/2.83M [00:01<00:00, 1.77MB/s]\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline('text-generation', model='skt/kogpt2-base-v2', max_length = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '나는 오늘 저녁에 밥을 먹고 내일 아침에 다시 한 번 일어나서 잠은 들어야 할 거 같습니다.\\n이런 식으로 정말 정말 열심히 살아가는 거 같은'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"나는 오늘 저녁에 밥을 먹고\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Downloading pytorch_model.bin: 100%|██████████| 496M/496M [00:32<00:00, 15.4MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 304/304 [00:00<00:00, 101kB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 1.05M/1.05M [00:00<00:00, 2.22MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 123/123 [00:00<00:00, 61.2kB/s]\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline('translation', model = 'circulus/kobart-trans-ko-en-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제로샷 분류 \n",
    "\n",
    "* 사전 학습된 모델에 의존하지 않고 분류 작업에 사용할 레이블을 특정할 수 있도록 함\n",
    "* 어떠한 레이블 세트에 대해서도 분류 작업을 수행 가능하도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to roberta-large-mnli and revision 130fb28 (https://huggingface.co/roberta-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 688/688 [00:00<00:00, 229kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 1.43G/1.43G [03:28<00:00, 6.82MB/s]\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 1.31MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.06MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 4.95MB/s]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('zero-shot-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business'],\n",
       " 'scores': [0.9725672006607056, 0.02743285521864891]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    \"This is a course about the Transformers library\" ,\n",
    "    candidate_labels = ['education', 'business']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.38k/1.38k [00:00<00:00, 615kB/s]\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model hyunwoongko/kobart with any of the following classes: (<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSequenceClassification'>, <class 'transformers.models.bart.modeling_tf_bart.TFBartForConditionalGeneration'>). See the original errors:\n\nwhile loading with TFAutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\Anaconda3\\envs\\llm_env\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\ProgramData\\Anaconda3\\envs\\llm_env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 565, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\ProgramData\\Anaconda3\\envs\\llm_env\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2823, in from_pretrained\n    raise EnvironmentError(\nOSError: hyunwoongko/kobart does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.\n\nwhile loading with TFBartForConditionalGeneration, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\Anaconda3\\envs\\llm_env\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\ProgramData\\Anaconda3\\envs\\llm_env\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2823, in from_pretrained\n    raise EnvironmentError(\nOSError: hyunwoongko/kobart does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m classifier \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mzero-shot-classification\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhyunwoongko/kobart\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\lib\\site-packages\\transformers\\pipelines\\__init__.py:834\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m framework \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m--> 834\u001b[0m     framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    835\u001b[0m         model,\n\u001b[0;32m    836\u001b[0m         model_classes\u001b[39m=\u001b[39mmodel_classes,\n\u001b[0;32m    837\u001b[0m         config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m    838\u001b[0m         framework\u001b[39m=\u001b[39mframework,\n\u001b[0;32m    839\u001b[0m         task\u001b[39m=\u001b[39mtask,\n\u001b[0;32m    840\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[0;32m    841\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    842\u001b[0m     )\n\u001b[0;32m    844\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    845\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\llm_env\\lib\\site-packages\\transformers\\pipelines\\base.py:282\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[39mfor\u001b[39;00m class_name, trace \u001b[39min\u001b[39;00m all_traceback\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    281\u001b[0m             error \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwhile loading with \u001b[39m\u001b[39m{\u001b[39;00mclass_name\u001b[39m}\u001b[39;00m\u001b[39m, an error is thrown:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtrace\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 282\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    283\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not load model \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m with any of the following classes: \u001b[39m\u001b[39m{\u001b[39;00mclass_tuple\u001b[39m}\u001b[39;00m\u001b[39m. See the original errors:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00merror\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m         )\n\u001b[0;32m    286\u001b[0m \u001b[39mif\u001b[39;00m framework \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    287\u001b[0m     framework \u001b[39m=\u001b[39m infer_framework(model\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model hyunwoongko/kobart with any of the following classes: (<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSequenceClassification'>, <class 'transformers.models.bart.modeling_tf_bart.TFBartForConditionalGeneration'>). See the original errors:\n\nwhile loading with TFAutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\Anaconda3\\envs\\llm_env\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\ProgramData\\Anaconda3\\envs\\llm_env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 565, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\ProgramData\\Anaconda3\\envs\\llm_env\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2823, in from_pretrained\n    raise EnvironmentError(\nOSError: hyunwoongko/kobart does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.\n\nwhile loading with TFBartForConditionalGeneration, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\Anaconda3\\envs\\llm_env\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\ProgramData\\Anaconda3\\envs\\llm_env\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2823, in from_pretrained\n    raise EnvironmentError(\nOSError: hyunwoongko/kobart does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.\n\n\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model='hyunwoongko/kobart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': '개구리는 양서류이다.',\n",
       " 'labels': ['양서류', '어류', '포유류'],\n",
       " 'scores': [0.6509882211685181, 0.22837843000888824, 0.12063339352607727]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    \"개구리는 양서류이다.\",\n",
    "    candidate_labels = ['어류', '포유류', '양서류']\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
